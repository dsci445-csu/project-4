---
title: "Data Cleaning"
author: "Dawson Carney"
date: "2025-11-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Alkalinity and Hardness Data

```{r}
library(dplyr)
library(tidyr)

# Match dates in alkalinity and hardness data
alk <- read.csv("RWAR_alkalinity.csv")
hardness <- read.csv("RWAR_hardness.csv")

# split DateStamp into day and time
alk_new <- separate(alk, col = DateStamp, into = c("Date", "Time"), sep = " ") |>
  select(-c("Time")) |>
  rename(alkalinity = Alkalinity..mg.L)
hardness_new <- separate(hardness, col = DateStamp, into = c("Date", "Time"), sep = " ") |>
  select(-c("Time")) |>
  rename(hardness = Hardness..mg.L)

# merge by Date
merged <- merge(alk_new, hardness_new, by="Date")

summary(merged) # Both alkalinity and hardness have zero-values (no data)

merged_clean1 <- merged[(merged$alkalinity != 0) & (merged$hardness != 0),] # remove

summary(merged_clean1) # no zero values, still a very large alkalinity value

tail(sort(merged_clean1$alkalinity)) # two significant outliers (equipment likely)

merged_clean2 <- merged_clean1[merged_clean1$alkalinity < 120, ] # remove outliers

summary(merged_clean2) # successfully removed zero values and outliers

```

```{r}
# output merged data to CSV (run after processing)

library(readr)
write_csv(merged_clean2, "alkalinity_hardness_cleaned.csv")

```

## Raw Water Data

```{r}

library(lubridate)
library(ggplot2)
library(zoo)
library(purrr)


## Import data, convert timestamp
rwar <- read.csv("RWAR_turb_cond_counts_pH_temp.csv")
rwar2 <- rwar |> mutate(DateTime = mdy_hm(rwar$Timestamp))
summary(rwar2)


## Temperature Data
# Plot
ggplot(rwar2) + geom_point(aes(x=DateTime, y=Temperature..deg.C))
# Find region of approx. zero temperature (outliers), set to NA
# Interpolate between nearest non-NA temperatures where temp is NA
rwar2_temp <- rwar2 |>
  select(c("DateTime", Temperature..deg.C)) |>
  mutate(temp_cleaned = ifelse(Temperature..deg.C < 1, NA, Temperature..deg.C)) |>
  mutate(temp_interp = na.approx(temp_cleaned, na.rm = FALSE))
# Average by day
rwar2_temp_daily <- rwar2_temp |>
  mutate(Day=date(DateTime)) |>
  group_by(Day) |>
  summarise(mean_Temp = mean(temp_interp))
ggplot(rwar2_temp_daily) + geom_point(aes(x=Day, y=mean_Temp))


## pH Data
# Plot
ggplot(rwar2) + geom_point(aes(x=DateTime, y=pH..SU))
# Does not seem like there are huge outliers, so average by day
rwar2_pH_daily <- rwar2 |>
  select(c("DateTime", pH..SU)) |>
  mutate(Day=date(DateTime)) |>
  group_by(Day) |>
  summarise(mean_pH = mean(pH..SU))
ggplot(rwar2_pH_daily) + geom_point(aes(x=Day, y=mean_pH))


## Conductivity Data
# Plot
ggplot(rwar2) + geom_point(aes(x=DateTime, y=Conductivity..mS.cm.1))
# Remove obvious outliers (only a few measurements; does not remove full days)
rwar2_cond <- rwar2 |> 
  select(c("DateTime", Conductivity..mS.cm.1)) |>
  filter(Conductivity..mS.cm.1 < 2 & Conductivity..mS.cm.1 > 0.2)
ggplot(rwar2_cond) + geom_point(aes(x=DateTime, y=Conductivity..mS.cm.1))
# Average by Day
rwar2_cond_daily <- rwar2_cond |>
  mutate(Day=date(DateTime)) |>
  group_by(Day) |>
  summarise(mean_Conductivity = mean(Conductivity..mS.cm.1))
ggplot(rwar2_cond_daily) + geom_point(aes(x=Day, y=mean_Conductivity))


## Turbidity Data
# Plot
ggplot(rwar2) + geom_point(aes(x=DateTime, y=Turbidity..NTU))
# Remove obvious outliers (only a few measurements; does not remove full days)
# Main obvious outliers are in mid 2019 and 2020, and are all above 7 NTU
rwar2_turb <- rwar2 |> 
  select(c("DateTime", Turbidity..NTU)) |>
  filter(Turbidity..NTU < 7)
ggplot(rwar2_turb) + geom_point(aes(x=DateTime, y=Turbidity..NTU))
# Average by Day
rwar2_turb_daily <- rwar2_turb |>
  mutate(Day=date(DateTime)) |>
  group_by(Day) |>
  summarise(mean_Turbidity = mean(Turbidity..NTU))
ggplot(rwar2_turb_daily) + geom_point(aes(x=Day, y=mean_Turbidity))

## Merge daily temp, pH, conductivity, turbidity

list_daily_data <- list(rwar2_temp_daily,
                        rwar2_pH_daily, 
                        rwar2_cond_daily,
                        rwar2_turb_daily)
merged_daily_data <- list_daily_data |>
  reduce(full_join, by = "Day") |>
  rename(Timestamp = Day)

```

```{r}
## Export CSV of cleaned raw water data

#write_csv(merged_daily_data, )
```

```{r}
# Merge dosing data to this data. Have significant overlap
dosing <- read.csv("CoagulantDoses_2017-20.csv") |>
  rename(Timestamp = Date) |> 
  mutate(Timestamp = mdy(dosing$Timestamp))

RWAR_dosing_cleaned <- inner_join(dosing, merged_daily_data)
summary(RWAR_dosing_cleaned) #can export to csv with same name

```



```{r}
water_lm <- lm(Ferric.chloride..mg.L ~ ., 
               data=RWAR_dosing_cleaned |> select(-c(Timestamp, Cationic.Polimer..mg.L)))
summary(water_lm)

```

```{r}
data <- read.csv("RWAR_dosing_cleaned.csv")
rwar_dosing_lag <- data |>
  mutate(Temp_lag1 = lag(mean_Temp, 1), 
         Temp_lag2 = lag(mean_Temp, 2),
         pH_lag1 = lag(mean_pH, 1),
         pH_lag2 = lag(mean_pH, 2),
         Cond_lag1 = lag(mean_Conductivity, 1),
         Cond_lag2 = lag(mean_Conductivity, 2),
         Turb_lag1 = lag(mean_Turbidity, 1),
         Turb_lag2 = lag(mean_Turbidity, 2),
         FeCl_change = Ferric.chloride..mg.L - lag(Ferric.chloride..mg.L),
         Polymer_change = Cationic.Polimer..mg.L - lag(Cationic.Polimer..mg.L))

# Get rid of first two observations (have NA values now)
rwar_dosing_lag1 <- rwar_dosing_lag[3:nrow(lagged_data),]

class_df <- rwar_dosing_lag1

threshold_smallchange <- 0.01

# normal data
class_df$FeCl_change <- ifelse(
    test = abs(class_df$FeCl_change) < threshold_smallchange,
    yes = "same",
    no = ifelse(
        test = class_df$FeCl_change < 0,
        yes = "dec",
        no = "inc"
    )
)

# polymer data
class_df$Polymer_change <- ifelse(
    test = abs(class_df$Polymer_change) < threshold_smallchange,
    yes = "same",
    no = ifelse(
        test = class_df$Polymer_change < 0,
        yes = "dec",
        no = "inc"
    )
)




# # Temp
# lagged_data$Temp_lag1[1] <- lagged_data$Temp_lag1[2]
# lagged_data$Temp_lag2[1:2] <- lagged_data$Temp_lag2[3]
# # pH
# lagged_data$pH_lag1[1] <- lagged_data$pH_lag1[2]
# lagged_data$pH_lag2[1:2] <- lagged_data$pH_lag2[3]
# # Conductivity
# lagged_data$Cond_lag1[1] <- lagged_data$Cond_lag1[2]
# lagged_data$Cond_lag2[1:2] <- lagged_data$Cond_lag2[3]
# # Turbidity
# lagged_data$Turb_lag1[1] <- lagged_data$Turb_lag1[2]
# lagged_data$Turb_lag2[1:2] <- lagged_data$Turb_lag2[3]


lm_fit <- lm(Ferric.chloride..mg.L ~ ., data=lagged_data1 |> select(-c(Timestamp)))
summary(lm_fit)

```

```{r}
write_csv(class_df, "RWAR_dosing_lag_incdec.csv")
```


### Classification

```{r}

# Pull in data
df_lag_incdec <- read.csv("RWAR_dosing_lag_incdec.csv") |> 
  mutate(Timestamp = ymd(Timestamp))
df_lag_incdec$FeCl_change <- as.factor(df_lag_incdec$FeCl_change)
df_lag_incdec$Polymer_change <- as.factor(df_lag_incdec$Polymer_change)


# Look at average water characteristics for each FeCl dose change group
df_lag_incdec |>
  select(-c(Timestamp)) |>
  group_by(FeCl_change) |>
  summarise(count = n(),
            mean_Temp = mean(mean_Temp),
            mean_pH = mean(mean_pH),
            mean_cond = mean(mean_Conductivity),
            mean_turbidity = mean(mean_Turbidity))

# Look at average water characteristics for each polymer dose change group
df_lag_incdec |>
  select(-c(Timestamp)) |>
  group_by(Polymer_change) |>
  summarise(count = n(),
            mean_Temp = mean(mean_Temp),
            mean_pH = mean(mean_pH),
            mean_cond = mean(mean_Conductivity),
            mean_turbidity = mean(mean_Turbidity))

# counts for FeCl change
summary(df_lag_incdec$FeCl_change)
# counts for polymer change
summary(df_lag_incdec$Polymer_change)

```

```{r}

## TRAIN/TEST DATA
train_df <- df_lag_incdec |> filter(year(Timestamp) <= 2019)
test_df  <- df_lag_incdec |> filter(year(Timestamp) == 2020)


## RANDOM FOREST

rf_spec <- rand_forest(mtry = sqrt(.cols())) |>
  set_engine("randomForest", importance = TRUE) |>
  set_mode("classification")

rf_fit <- rf_spec |>
  fit(high_sales ~ ., data = df_train)

vip(rf_fit) ## still Price and Shelf location are the most important

rf_fit |>
  augment(new_data = df_test) |>
  accuracy(truth = high_sales, estimate = .pred_class)

# 5. Compare the OOB confusion matrix to your test confusion matrix. [**Hint:** The `confusion` element of the model output fit is OOB.]

## OOB:
rf_fit |>
  extract_fit_engine() %>%
  .$confusion

## test confusion
rf_fit |>
  augment(new_data = df_test) |>
  conf_mat(truth = high_sales, estimate = .pred_class)
```




```{r}
# Look at performance of a model that simply pulls previous year dose
# not working

dosing <- read_csv("CoagulantDoses_2017-20.csv")
dosing <- dosing |> 
  mutate(Timestamp = mdy(dosing$Date)) |>
  select(-c(Date))

df_lag_incdec_prevdose <- df_lag_incdec |>
  rowwise() |>
  mutate(
    target_time = Timestamp - years(1),
    FeCl_prev = dosing$Ferric.chloride..mg.L[which.min(abs(dosing$Timestamp - target_time))],
    Polymer_prev = dosing$`Cationic Polimer, mg/L`[which.min(abs(dosing$Timestamp - target_time))]) |>
  ungroup()

```


