---
title: "Data Cleaning"
author: "Dawson Carney"
date: "2025-11-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=F}
library(ISLR) ## data package
library(tidyverse) ## data manipulation
library(tidymodels) ## tidy modeling
library(knitr) ## tables
library(rpart.plot) ## tree diagrams
library(vip) ## plotting variable importance
library(dplyr)
library(tidyr)
library(readr)
library(lubridate)
library(ggplot2)
library(zoo)
library(purrr)
library(knitr)
library(patchwork) ## plot arrangement
library(parsnip)
library(discrim)
library(nnet)
library(broom)
library(dplyr)
library(kableExtra)
library(tibble)

```



## Alkalinity and Hardness Data

```{r}
library(dplyr)
library(tidyr)

# Match dates in alkalinity and hardness data
alk <- read.csv("RWAR_alkalinity.csv")
hardness <- read.csv("RWAR_hardness.csv")

# split DateStamp into day and time
alk_new <- separate(alk, col = DateStamp, into = c("Date", "Time"), sep = " ") |>
  select(-c("Time")) |>
  rename(alkalinity = Alkalinity..mg.L)
hardness_new <- separate(hardness, col = DateStamp, into = c("Date", "Time"), sep = " ") |>
  select(-c("Time")) |>
  rename(hardness = Hardness..mg.L)

# merge by Date
merged <- merge(alk_new, hardness_new, by="Date")

summary(merged) # Both alkalinity and hardness have zero-values (no data)

merged_clean1 <- merged[(merged$alkalinity != 0) & (merged$hardness != 0),] # remove

summary(merged_clean1) # no zero values, still a very large alkalinity value

tail(sort(merged_clean1$alkalinity)) # two significant outliers (equipment likely)

merged_clean2 <- merged_clean1[merged_clean1$alkalinity < 120, ] # remove outliers

summary(merged_clean2) # successfully removed zero values and outliers

```

```{r}
# output merged data to CSV (run after processing)

library(readr)
write_csv(merged_clean2, "alkalinity_hardness_cleaned.csv")

```

## Raw Water Data

```{r}

library(lubridate)
library(ggplot2)
library(zoo)
library(purrr)


## Import data, convert timestamp
rwar <- read.csv("RWAR_turb_cond_counts_pH_temp.csv")
rwar2 <- rwar |> mutate(DateTime = mdy_hm(rwar$Timestamp))
summary(rwar2)


## Temperature Data
# Plot
ggplot(rwar2) + geom_point(aes(x=DateTime, y=Temperature..deg.C))
# Find region of approx. zero temperature (outliers), set to NA
# Interpolate between nearest non-NA temperatures where temp is NA
rwar2_temp <- rwar2 |>
  select(c("DateTime", Temperature..deg.C)) |>
  mutate(temp_cleaned = ifelse(Temperature..deg.C < 1, NA, Temperature..deg.C)) |>
  mutate(temp_interp = na.approx(temp_cleaned, na.rm = FALSE))
# Average by day
rwar2_temp_daily <- rwar2_temp |>
  mutate(Day=date(DateTime)) |>
  group_by(Day) |>
  summarise(mean_Temp = mean(temp_interp))
ggplot(rwar2_temp_daily) + geom_point(aes(x=Day, y=mean_Temp))


## pH Data
# Plot
ggplot(rwar2) + geom_point(aes(x=DateTime, y=pH..SU))
# Does not seem like there are huge outliers, so average by day
rwar2_pH_daily <- rwar2 |>
  select(c("DateTime", pH..SU)) |>
  mutate(Day=date(DateTime)) |>
  group_by(Day) |>
  summarise(mean_pH = mean(pH..SU))
ggplot(rwar2_pH_daily) + geom_point(aes(x=Day, y=mean_pH))


## Conductivity Data
# Plot
ggplot(rwar2) + geom_point(aes(x=DateTime, y=Conductivity..mS.cm.1))
# Remove obvious outliers (only a few measurements; does not remove full days)
rwar2_cond <- rwar2 |> 
  select(c("DateTime", Conductivity..mS.cm.1)) |>
  filter(Conductivity..mS.cm.1 < 2 & Conductivity..mS.cm.1 > 0.2)
ggplot(rwar2_cond) + geom_point(aes(x=DateTime, y=Conductivity..mS.cm.1))
# Average by Day
rwar2_cond_daily <- rwar2_cond |>
  mutate(Day=date(DateTime)) |>
  group_by(Day) |>
  summarise(mean_Conductivity = mean(Conductivity..mS.cm.1))
ggplot(rwar2_cond_daily) + geom_point(aes(x=Day, y=mean_Conductivity))


## Turbidity Data
# Plot
ggplot(rwar2) + geom_point(aes(x=DateTime, y=Turbidity..NTU))
# Remove obvious outliers (only a few measurements; does not remove full days)
# Main obvious outliers are in mid 2019 and 2020, and are all above 7 NTU
rwar2_turb <- rwar2 |> 
  select(c("DateTime", Turbidity..NTU)) |>
  filter(Turbidity..NTU < 7)
ggplot(rwar2_turb) + geom_point(aes(x=DateTime, y=Turbidity..NTU))
# Average by Day
rwar2_turb_daily <- rwar2_turb |>
  mutate(Day=date(DateTime)) |>
  group_by(Day) |>
  summarise(mean_Turbidity = mean(Turbidity..NTU))
ggplot(rwar2_turb_daily) + geom_point(aes(x=Day, y=mean_Turbidity))

## Merge daily temp, pH, conductivity, turbidity

list_daily_data <- list(rwar2_temp_daily,
                        rwar2_pH_daily, 
                        rwar2_cond_daily,
                        rwar2_turb_daily)
merged_daily_data <- list_daily_data |>
  reduce(full_join, by = "Day") |>
  rename(Timestamp = Day)

```

```{r}
## Export CSV of cleaned raw water data

#write_csv(merged_daily_data, )
```

```{r}
# Merge dosing data to this data. Have significant overlap
dosing <- read.csv("CoagulantDoses_2017-20.csv") |>
  rename(Timestamp = Date) |> 
  mutate(Timestamp = mdy(dosing$Timestamp))

RWAR_dosing_cleaned <- inner_join(dosing, merged_daily_data)
summary(RWAR_dosing_cleaned) #can export to csv with same name

```



```{r}
water_lm <- lm(Ferric.chloride..mg.L ~ ., 
               data=RWAR_dosing_cleaned |> select(-c(Timestamp, Cationic.Polimer..mg.L)))
summary(water_lm)

```

```{r}
data <- read.csv("RWAR_dosing_cleaned.csv")
rwar_dosing_lag <- data |>
  mutate(Temp_lag1 = lag(mean_Temp, 1), 
         Temp_lag2 = lag(mean_Temp, 2),
         pH_lag1 = lag(mean_pH, 1),
         pH_lag2 = lag(mean_pH, 2),
         Cond_lag1 = lag(mean_Conductivity, 1),
         Cond_lag2 = lag(mean_Conductivity, 2),
         Turb_lag1 = lag(mean_Turbidity, 1),
         Turb_lag2 = lag(mean_Turbidity, 2),
         FeCl_change = Ferric.chloride..mg.L - lag(Ferric.chloride..mg.L),
         Polymer_change = Cationic.Polimer..mg.L - lag(Cationic.Polimer..mg.L))

# Get rid of first two observations (have NA values now)
rwar_dosing_lag1 <- rwar_dosing_lag[3:nrow(lagged_data),]

## Set up a column of inc/dec for 
class_df <- rwar_dosing_lag1
threshold_smallchange <- 0.01 # cutoff for dose changes that are approx. zero

# normal data
class_df$FeCl_change <- ifelse(
    test = abs(class_df$FeCl_change) < threshold_smallchange,
    yes = "same",
    no = ifelse(
        test = class_df$FeCl_change < 0,
        yes = "dec",
        no = "inc"
    )
)

# polymer data
class_df$Polymer_change <- ifelse(
    test = abs(class_df$Polymer_change) < threshold_smallchange,
    yes = "same",
    no = ifelse(
        test = class_df$Polymer_change < 0,
        yes = "dec",
        no = "inc"
    )
)

```

```{r}
write_csv(class_df, "RWAR_dosing_lag_incdec.csv")
```


### Classification

```{r}

# Pull in data
df_lag_incdec <- read.csv("RWAR_dosing_lag_incdec.csv") |> 
  mutate(Timestamp = ymd(Timestamp))
df_lag_incdec$FeCl_change <- as.factor(df_lag_incdec$FeCl_change)
df_lag_incdec$Polymer_change <- as.factor(df_lag_incdec$Polymer_change)


# Look at average water characteristics for each FeCl dose change group
df_lag_incdec |>
  select(-c(Timestamp)) |>
  group_by(FeCl_change) |>
  summarise(count = n(),
            mean_Temp = mean(mean_Temp),
            mean_pH = mean(mean_pH),
            mean_cond = mean(mean_Conductivity),
            mean_turbidity = mean(mean_Turbidity))

# Look at average water characteristics for each polymer dose change group
df_lag_incdec |>
  select(-c(Timestamp)) |>
  group_by(Polymer_change) |>
  summarise(count = n(),
            mean_Temp = mean(mean_Temp),
            mean_pH = mean(mean_pH),
            mean_cond = mean(mean_Conductivity),
            mean_turbidity = mean(mean_Turbidity))

# counts for FeCl change
summary(df_lag_incdec$FeCl_change)
# counts for polymer change
summary(df_lag_incdec$Polymer_change)

```

```{r}
## SET SEED
set.seed(445)

## TRAIN/TEST DATA (2018-2019 and 2020)
df_full <- df_lag_incdec |> select(-c(FeCl_dose, Polymer_dose))
df_train <- df_full |> filter(year(Timestamp) <= 2019)
df_test  <- df_full |> filter(year(Timestamp) == 2020)
```


```{r}
set.seed(445)

## RANDOM FOREST
rf_spec <- rand_forest(mtry = sqrt(.cols()), trees=500, min_n=1) |>
  set_engine("randomForest", importance = TRUE) |>
  set_mode("classification")

# Different model fits
rf_fit1 <- rf_spec |>
  fit(FeCl_change ~ ., data = df_train |> 
        select(-c(Polymer_change, Timestamp)))
rf_fit2 <- rf_spec |>
  fit(FeCl_change ~ mean_Temp + mean_pH + Temp_lag1 + Temp_lag2 + pH_lag1 + pH_lag2, 
      data = df_train)
rf_fit3 <- rf_spec |>
  fit(FeCl_change ~ mean_pH + mean_Temp, 
      data = df_train)

# Model fit to take performance metrics of:
rf_fit <- rf_fit3

# Most important predictors
vip(rf_fit)

## Accuracy
rf_fit |>
  augment(new_data = df_test) |>
  accuracy(truth = FeCl_change, estimate = .pred_class)
## Test confusion matrix
rf_fit |>
  augment(new_data = df_test) |>
  conf_mat(truth = FeCl_change, estimate = .pred_class)

## Notes
# 34% accuracy for all predictors
# 46.6% accuracy for just temperature and pH

```

```{r}
## set seed
set.seed(445)

## LDA
lda_spec <- discrim_linear()
lda_fit1 <- lda_spec |>
  fit(FeCl_change ~ ., data = df_train |> 
        select(-c(Polymer_change, Timestamp)))
lda_fit2 <- lda_spec |>
  fit(FeCl_change ~ mean_pH + mean_Temp, data = df_train)

# select LDA to take metrics of
lda_fit <- lda_fit2

# Test confusion matrix
lda_fit |> 
  augment(new_data = df_test) |>
  conf_mat(truth = FeCl_change, estimate = .pred_class)

# Test accuracy
lda_fit |>
  augment(new_data = df_test) |>
  accuracy(truth = FeCl_change, estimate = .pred_class)

## Notes
# 39% test accuracy on all predictors
# 41% test accuracy on temp and pH


## QDA
qda_spec <- discrim_quad()
qda_fit1 <- qda_spec |>
  fit(FeCl_change ~ ., data = df_train |> 
        select(-c(Polymer_change, Timestamp)))
qda_fit2 <- qda_spec |>
  fit(FeCl_change ~ mean_pH + mean_Temp, data = df_train)

# select LDA to take metrics of
qda_fit <- qda_fit2

# Test confusion matrix
qda_fit |> 
  augment(new_data = df_test) |>
  conf_mat(truth = FeCl_change, estimate = .pred_class)

# Test accuracy
qda_fit |>
  augment(new_data = df_test) |>
  accuracy(truth = FeCl_change, estimate = .pred_class)

## Notes
# 39.5% test accuracy on all predictors
# 43.7% test accuracy on temp and pH

```

```{r}

## MULTINOMIAL REGRESSION
multi_model <- multinom_reg() |>
  set_engine("nnet") |>
  set_mode("classification")

multi_recipe1 <- recipe(FeCl_change ~ mean_Temp + Temp_lag1 + Temp_lag2 + mean_pH + pH_lag1 + pH_lag2 + mean_Conductivity + Cond_lag1 + Cond_lag2, data = df_train) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

multi_recipe <- multi_recipe1

multi_workflow <- workflow() |>
  add_recipe(multi_recipe) |>
  add_model(multi_model)

multi_fit <- fit(multi_workflow, data = df_train)

df_pred_class <- predict(multi_fit, new_data = df_test, type = "class") |>
  bind_cols(df_test |> select(FeCl_change))
df_pred_probs <- predict(multi_fit, new_data = df_test, type = "prob") |>
  bind_cols(df_test |> select(FeCl_change))

accuracy_result <- metrics(df_pred_class, truth = FeCl_change, estimate = .pred_class) |> 
  filter(.metric == "accuracy")
confusion_matrix <- conf_mat(df_pred_class, truth = FeCl_change, estimate = .pred_class)

# results
print(accuracy_result)
print(confusion_matrix) |> kable()

multi_fit |> extract_fit_engine()

## NOTES
# turbidity is highly variable and does not help

```


```{r}
library(tidymodels)
library(dplyr)
library(lubridate)

df_modeling <- df_lag_incdec |> 
  mutate(day = yday(Timestamp))

lm_model_spec <- linear_reg() |>
  set_engine("lm") |>
  set_mode("regression")

lm_recipe <- 
  recipe(FeCl_dose ~ day, data = df_modeling) |>
  step_poly(day, degree = 10)

lm_workflow <- workflow() |>
  add_model(lm_model_spec) |>
  add_recipe(lm_recipe)

lm_fit_result <- fit(lm_workflow, data = df_modeling)

predictions <- predict(lm_fit_result, new_data = df_modeling) |>
  bind_cols(df_modeling |> select(FeCl_dose))

rmse_result <- predictions |>
  rmse(truth = FeCl_dose, estimate = .pred)

mae_result <- predictions |>
  mae(truth = FeCl_dose, estimate = .pred)

print(rmse_result)
print(mae_result)


```



```{r}
# Look at performance of a model that simply pulls previous year dose
# not working

dosing <- read_csv("CoagulantDoses_2017-20.csv")
dosing <- dosing |> 
  mutate(Timestamp = mdy(dosing$Date)) |>
  select(-c(Date))

df_lag_incdec_prevdose <- df_lag_incdec |>
  rowwise() |>
  mutate(
    target_time = Timestamp - years(1),
    FeCl_prev = dosing$Ferric.chloride..mg.L[which.min(abs(dosing$Timestamp - target_time))],
    Polymer_prev = dosing$`Cationic Polimer, mg/L`[which.min(abs(dosing$Timestamp - target_time))]) |>
  ungroup()

```


